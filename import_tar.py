#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Docker taræ–‡ä»¶å¯¼å…¥å·¥å…·
ä»ç°æœ‰çš„Docker taræ–‡ä»¶ä¸­æå–layerså¹¶æ·»åŠ åˆ°ç¼“å­˜ç›®å½•
"""

import os
import sys
import json
import hashlib
import shutil
import tarfile
import argparse
from pathlib import Path

def calculate_layer_digest(layer_tar_path: str) -> str:
    """Calculate SHA256 digest of a layer tar file"""
    sha256_hash = hashlib.sha256()
    with open(layer_tar_path, 'rb') as f:
        for chunk in iter(lambda: f.read(8192), b""):
            sha256_hash.update(chunk)
    return f"sha256:{sha256_hash.hexdigest()}"

def format_speed(bytes_size):
    """Format file size in human-readable format"""
    if bytes_size < 1024:
        return f"{bytes_size:.0f} B"
    elif bytes_size < 1024 * 1024:
        return f"{bytes_size/1024:.1f} KB"
    elif bytes_size < 1024 * 1024 * 1024:
        return f"{bytes_size/(1024*1024):.1f} MB"
    else:
        return f"{bytes_size/(1024*1024*1024):.1f} GB"

def get_layer_cache_path(layer_digest: str, layers_cache_dir: Path) -> Path:
    """Get the cache path for a layer based on its digest"""
    return layers_cache_dir / layer_digest.replace(':', '_')

def check_layer_cache(layer_digest: str, layers_cache_dir: Path) -> bool:
    """Check if a layer exists in cache"""
    cache_path = get_layer_cache_path(layer_digest, layers_cache_dir)
    layer_file = cache_path / 'layer.tar'
    return layer_file.exists()

def save_layer_to_cache(layer_digest: str, layer_tar_path: str, layers_cache_dir: Path) -> bool:
    """Save a layer to cache"""
    try:
        cache_path = get_layer_cache_path(layer_digest, layers_cache_dir)
        cache_path.mkdir(parents=True, exist_ok=True)
        
        # Create hard link to save space
        cache_layer_file = cache_path / 'layer.tar'
        if not cache_layer_file.exists():
            os.link(layer_tar_path, cache_layer_file)
        
        # Save metadata
        metadata = {
            'digest': layer_digest,
            'size': os.path.getsize(layer_tar_path),
            'cached_at': __import__('time').time()
        }
        with open(cache_path / 'metadata.json', 'w') as f:
            json.dump(metadata, f)
        
        return True
    except Exception as e:
        print(f"Warning: Failed to cache layer {layer_digest[7:19]}: {e}")
        return False

def import_docker_tar_to_cache(tar_file_path: str, cache_dir: Path):
    """Import layers from a Docker tar file to cache"""
    print(f"ğŸ”„ å¼€å§‹å¯¼å…¥Docker taræ–‡ä»¶åˆ°ç¼“å­˜: {tar_file_path}")
    
    if not os.path.exists(tar_file_path):
        print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ {tar_file_path}")
        return
    
    layers_cache_dir = cache_dir / 'layers'
    layers_cache_dir.mkdir(parents=True, exist_ok=True)
    
    imported_count = 0
    skipped_count = 0
    total_size = 0
    temp_dir = Path('temp_docker_import')
    
    try:
        with tarfile.open(tar_file_path, 'r') as tar:
            # æå–åˆ°ä¸´æ—¶ç›®å½•
            if temp_dir.exists():
                shutil.rmtree(temp_dir)
            temp_dir.mkdir()
            
            print("ğŸ“¦ è§£å‹Docker taræ–‡ä»¶...")
            tar.extractall(temp_dir)
            
            # æŸ¥æ‰¾manifest.jsonæ–‡ä»¶
            manifest_files = list(temp_dir.glob('**/manifest.json'))
            if not manifest_files:
                print("âŒ é”™è¯¯: åœ¨Docker taræ–‡ä»¶ä¸­æœªæ‰¾åˆ°manifest.json")
                return
            
            manifest_file = manifest_files[0]
            with open(manifest_file, 'r') as f:
                manifest_data = json.load(f)
            
            print(f"ğŸ“‹ æ‰¾åˆ° {len(manifest_data)} ä¸ªé•œåƒæ¸…å•")
            
            # å¤„ç†æ¯ä¸ªé•œåƒçš„layers
            for image_manifest in manifest_data:
                if 'Layers' not in image_manifest:
                    continue
                    
                repo_tags = image_manifest.get('RepoTags', ['unknown:latest'])
                print(f"\nğŸ·ï¸  å¤„ç†é•œåƒ: {', '.join(repo_tags)}")
                
                layers = image_manifest['Layers']
                print(f"ğŸ“¦ å‘ç° {len(layers)} ä¸ªå±‚")
                
                for layer_path in layers:
                    full_layer_path = temp_dir / layer_path
                    if not full_layer_path.exists():
                        print(f"âš ï¸  è­¦å‘Š: å±‚æ–‡ä»¶ä¸å­˜åœ¨ {layer_path}")
                        continue
                    
                    # è®¡ç®—å±‚çš„digest
                    print(f"ğŸ” è®¡ç®—å±‚digest: {layer_path}")
                    layer_digest = calculate_layer_digest(str(full_layer_path))
                    
                    # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨ç¼“å­˜ä¸­
                    if check_layer_cache(layer_digest, layers_cache_dir):
                        print(f"â­ï¸  è·³è¿‡å·²ç¼“å­˜çš„å±‚: {layer_digest[7:19]}")
                        skipped_count += 1
                        continue
                    
                    # å¯¼å…¥åˆ°ç¼“å­˜
                    layer_size = full_layer_path.stat().st_size
                    if save_layer_to_cache(layer_digest, str(full_layer_path), layers_cache_dir):
                        print(f"âœ… æˆåŠŸå¯¼å…¥å±‚: {layer_digest[7:19]} ({format_speed(layer_size)})")
                        imported_count += 1
                        total_size += layer_size
                    else:
                        print(f"âŒ å¯¼å…¥å±‚å¤±è´¥: {layer_digest[7:19]}")
            
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            shutil.rmtree(temp_dir)
            
    except Exception as e:
        print(f"âŒ å¯¼å…¥è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        # æ¸…ç†ä¸´æ—¶ç›®å½•
        if temp_dir.exists():
            shutil.rmtree(temp_dir)
        return
    
    # æ˜¾ç¤ºå¯¼å…¥ç»Ÿè®¡
    print(f"\nğŸ“Š å¯¼å…¥å®Œæˆç»Ÿè®¡:")
    print(f"   âœ… æˆåŠŸå¯¼å…¥: {imported_count} ä¸ªå±‚")
    print(f"   â­ï¸  è·³è¿‡å·²å­˜åœ¨: {skipped_count} ä¸ªå±‚")
    if total_size > 0:
        if total_size >= 1024 * 1024 * 1024:
            print(f"   ğŸ’¾ å¯¼å…¥æ•°æ®é‡: {total_size/(1024*1024*1024):.1f} GB")
        else:
            print(f"   ğŸ’¾ å¯¼å…¥æ•°æ®é‡: {total_size/(1024*1024):.1f} MB")
    print(f"   ğŸ“ ç¼“å­˜ä½ç½®: {layers_cache_dir}")
    print(f"\nğŸ‰ Docker taræ–‡ä»¶å¯¼å…¥å®Œæˆï¼")

def main():
    parser = argparse.ArgumentParser(
        description='ä»Docker taræ–‡ä»¶å¯¼å…¥layersåˆ°ç¼“å­˜ç›®å½•',
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument('tar_file', help='Docker taræ–‡ä»¶è·¯å¾„')
    parser.add_argument('--cache-dir', help='ç¼“å­˜ç›®å½• (é»˜è®¤: ./docker_images_cache)', default='./docker_images_cache')
    
    args = parser.parse_args()
    
    cache_dir = Path(args.cache_dir).expanduser().resolve()
    
    print("="*60)
    print("ğŸ³ Docker Tarå¯¼å…¥å·¥å…·")
    print("ğŸ“¦ å°†Docker taræ–‡ä»¶çš„layerså¯¼å…¥åˆ°ç¼“å­˜ç›®å½•")
    print("="*60)
    print()
    
    import_docker_tar_to_cache(args.tar_file, cache_dir)

if __name__ == '__main__':
    main()